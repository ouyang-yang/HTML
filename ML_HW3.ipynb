{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7922347761105573\n",
      "(100, 11)\n",
      "[ 0.29070963 -0.04988084  0.04893561 -0.08623605 -0.06658103  0.10689752\n",
      " -0.12356574  0.09486241  0.26696655 -0.15660245 -0.06382855]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"ML_HW3_data_train.dat\", sep = \"\\t\", header = None)\n",
    "df.insert(loc=0, column=\"x0\", value=1)\n",
    "df.columns =['x0', 'x1', 'x2', 'x3', 'x4', 'x5', 'x6', 'x7', 'x8', 'x9', 'x10', 'sign']\n",
    "\n",
    "X = df.iloc[:, :-1].to_numpy()\n",
    "y = df.iloc[:, -1].to_numpy()\n",
    "\n",
    "X_pinv = np.linalg.pinv(X)\n",
    "w_lin = X_pinv.dot(y)\n",
    "Ein_square = np.linalg.norm(X.dot(w_lin) - y)**2 / len(y)\n",
    "\n",
    "print(Ein_square)\n",
    "print(X.shape)\n",
    "print(w_lin)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average in-sample error square: 0.82232\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"ML_HW3_data_train.dat\", sep = \"\\t\", header = None)\n",
    "df.insert(loc=0, column=\"x0\", value=1)\n",
    "df.columns =['x0', 'x1', 'x2', 'x3', 'x4', 'x5', 'x6', 'x7', 'x8', 'x9', 'x10', 'sign']\n",
    "\n",
    "X = df.iloc[:, :-1].to_numpy()\n",
    "y = df.iloc[:, -1].to_numpy()\n",
    "\n",
    "eta = 0.001\n",
    "num_iter = 800 # Define the number of iterations\n",
    "num_exp = 1000 # Define the number of experiments\n",
    "\n",
    "Ein_sqr = []\n",
    "# Run the experiment for num_exp times with different random seeds\n",
    "for i in range(num_exp):\n",
    "    w = np.zeros(11) # Initialize weights to zeros(w0=0)\n",
    "    rng = np.random.default_rng() # Set the random seed\n",
    "\n",
    "    # Shuffle the data\n",
    "    perm = rng.permutation(len(y))\n",
    "    X_shuffled = X[perm]\n",
    "    y_shuffled = y[perm]\n",
    "\n",
    "    # Run SGD for num_iter iterations\n",
    "    for t in range(num_iter):\n",
    "        # Pick one example uniformly at random\n",
    "        j = rng.integers(len(y))\n",
    "        x_j = X_shuffled[j]\n",
    "        y_j = y_shuffled[j]\n",
    "        \n",
    "        # Compute the stochastic gradient\n",
    "        grad = 2 * x_j * (x_j.dot(w) - y_j)\n",
    "        \n",
    "        # Update the weights\n",
    "        w -= eta * grad\n",
    "    \n",
    "    # Print the in-sample error square for the final weights\n",
    "    error_square = np.linalg.norm(X.dot(w) - y)**2 / len(y)\n",
    "    Ein_sqr.append(error_square)\n",
    "\n",
    "\n",
    "avg_Ein_sqr = np.mean(Ein_sqr)\n",
    "print(f\"Average in-sample error square: {avg_Ein_sqr:.5f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average in-sample cross-entropy error: 0.69494\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"ML_HW3_data_train.dat\", sep = \"\\t\", header = None)\n",
    "df.insert(loc=0, column=\"x0\", value=1)\n",
    "df.columns =['x0', 'x1', 'x2', 'x3', 'x4', 'x5', 'x6', 'x7', 'x8', 'x9', 'x10', 'sign']\n",
    "\n",
    "X = df.iloc[:, :-1].to_numpy()\n",
    "y = df.iloc[:, -1].to_numpy()\n",
    "\n",
    "eta = 0.001\n",
    "num_iter = 800 # Define the number of iterations\n",
    "num_exp = 1000 # Define the number of experiments\n",
    "\n",
    "\n",
    "Ein_ce = []\n",
    "# Run the experiment for num_exp times with different random seeds\n",
    "for i in range(num_exp):\n",
    "    rng = np.random.default_rng()\n",
    "\n",
    "    # Initialize weights to zeros(w0=0)\n",
    "    w = np.zeros(11)\n",
    "\n",
    "    # Run SGD for num_iter iterations\n",
    "    for t in range(num_iter):\n",
    "        # Pick one example uniformly at random\n",
    "        j = rng.integers(len(y))\n",
    "        x_j = X[j]\n",
    "        y_j = y[j]\n",
    "\n",
    "        # Compute the sigmoid function\n",
    "        z = np.dot(w, x_j)\n",
    "        sigmoid = 1 / (1 + np.exp(-z))\n",
    "        \n",
    "        # Compute the stochastic gradient\n",
    "        grad = x_j * (sigmoid - y_j)\n",
    "        \n",
    "        # Update the weights\n",
    "        w -= eta * grad\n",
    "    \n",
    "    # Print the in-sample error square for the final weights\n",
    "    error_ce = np.log(1 + np.exp(-y * X.dot(w)))\n",
    "    Ein_ce.append(error_ce)\n",
    "\n",
    "avg_Ein_ce = np.mean(Ein_ce)\n",
    "print(f\"Average in-sample cross-entropy error: {avg_Ein_ce:.5f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 16: w0=wlin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average in-sample cross-entropy error: 0.65677\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"ML_HW3_data_train.dat\", sep = \"\\t\", header = None)\n",
    "df.insert(loc=0, column=\"x0\", value=1)\n",
    "df.columns =['x0', 'x1', 'x2', 'x3', 'x4', 'x5', 'x6', 'x7', 'x8', 'x9', 'x10', 'sign']\n",
    "\n",
    "X = df.iloc[:, :-1].to_numpy()\n",
    "y = df.iloc[:, -1].to_numpy()\n",
    "\n",
    "eta = 0.001\n",
    "num_iter = 800 # Define the number of iterations\n",
    "num_exp = 1000 # Define the number of experiments\n",
    "\n",
    "\n",
    "Ein_ce = []\n",
    "# Run the experiment for num_exp times with different random seeds\n",
    "for i in range(num_exp):\n",
    "    rng = np.random.default_rng()\n",
    "\n",
    "    # Initialize weights to zeros(w0=wlin)\n",
    "    X_pinv = np.linalg.pinv(X)\n",
    "    w = X_pinv.dot(y) \n",
    "\n",
    "    # Run SGD for num_iter iterations\n",
    "    for t in range(num_iter):\n",
    "        # Pick one example uniformly at random\n",
    "        j = rng.integers(len(y))\n",
    "        x_j = X[j]\n",
    "        y_j = y[j]\n",
    "\n",
    "        # Compute the sigmoid function\n",
    "        z = np.dot(w, x_j)\n",
    "        sigmoid = 1 / (1 + np.exp(-z))\n",
    "        \n",
    "        # Compute the stochastic gradient\n",
    "        grad = x_j * (sigmoid - y_j)\n",
    "        \n",
    "        # Update the weights\n",
    "        w -= eta * grad\n",
    "    \n",
    "    # Print the in-sample error square for the final weights\n",
    "    error_ce = np.log(1 + np.exp(-y * X.dot(w)))\n",
    "    Ein_ce.append(error_ce)\n",
    "\n",
    "avg_Ein_ce = np.mean(Ein_ce)\n",
    "print(f\"Average in-sample cross-entropy error: {avg_Ein_ce:.5f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average absolute gap between Ein and Eout: 0.03\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv(\"ML_HW3_data_train.dat\", sep = \"\\t\", header = None)\n",
    "df_train.insert(loc=0, column=\"x0\", value=1)\n",
    "df_train.columns =['x0', 'x1', 'x2', 'x3', 'x4', 'x5', 'x6', 'x7', 'x8', 'x9', 'x10', 'sign']\n",
    "X_train = df_train.iloc[:, :-1].to_numpy()\n",
    "y_train = df_train.iloc[:, -1].to_numpy()\n",
    "\n",
    "df_test = pd.read_csv(\"ML_HW3_data_test.dat\", sep = \"\\t\", header = None)\n",
    "df_test.insert(loc=0, column=\"x0\", value=1)\n",
    "df_test.columns =['x0', 'x1', 'x2', 'x3', 'x4', 'x5', 'x6', 'x7', 'x8', 'x9', 'x10', 'sign']\n",
    "X_test = df_test.iloc[:, :-1].to_numpy()\n",
    "y_test = df_test.iloc[:, -1].to_numpy()\n",
    "\n",
    "eta = 0.001\n",
    "num_iter = 800 # Define the number of iterations\n",
    "num_exp = 1000 # Define the number of experiments\n",
    "\n",
    "# Define the logistic regression function\n",
    "def logistic_regression(X, y, w, eta, num_iter):\n",
    "    for t in range(num_iter):\n",
    "        # Pick one example uniformly at random\n",
    "        j = rng.integers(len(y))\n",
    "        x_j = X[j]\n",
    "        y_j = y[j]\n",
    "\n",
    "        # Compute the sigmoid function\n",
    "        z = np.dot(w, x_j)\n",
    "        sigmoid = 1 / (1 + np.exp(-z))\n",
    "\n",
    "        # Compute the stochastic gradient\n",
    "        grad = x_j * (sigmoid - y_j)\n",
    "        \n",
    "        w -= eta * grad # Update the weights\n",
    "    return w\n",
    "\n",
    "# Define the classification error function\n",
    "def classification_error(X, y, w):\n",
    "    y_pred = np.sign(X.dot(w))\n",
    "    misclassified = (y_pred != y)\n",
    "    return np.mean(misclassified)\n",
    "\n",
    "results = []\n",
    "# Run the experiment for the specified number of iterations\n",
    "for i in range(num_exp):\n",
    "    rng = np.random.default_rng()\n",
    "    \n",
    "    # Initialize weights to zeros(w0=wlin)\n",
    "    X_pinv = np.linalg.pinv(X)\n",
    "    w = X_pinv.dot(y) \n",
    "\n",
    "    # Run logistic regression on the training set, update w\n",
    "    w = logistic_regression(X_train, y_train, w, eta, num_iter)\n",
    "\n",
    "    # Compute the in-sample classification error\n",
    "    Ein = classification_error(X_train, y_train, w)\n",
    "    # Compute the out-of-sample classification error\n",
    "    Eout = classification_error(X_test, y_test, w)\n",
    "\n",
    "    results.append(np.abs(Ein-Eout))\n",
    "\n",
    "avg_gap = np.mean(results)\n",
    "print(f\"Average absolute gap between Ein and Eout: {avg_gap:.2f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Absolute gap between Ein and Eout: 0.04\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv(\"ML_HW3_data_train.dat\", sep = \"\\t\", header = None)\n",
    "df_train.insert(loc=0, column=\"x0\", value=1)\n",
    "df_train.columns =['x0', 'x1', 'x2', 'x3', 'x4', 'x5', 'x6', 'x7', 'x8', 'x9', 'x10', 'sign']\n",
    "X_train = df_train.iloc[:, :-1].to_numpy()\n",
    "y_train = df_train.iloc[:, -1].to_numpy()\n",
    "\n",
    "df_test = pd.read_csv(\"ML_HW3_data_test.dat\", sep = \"\\t\", header = None)\n",
    "df_test.insert(loc=0, column=\"x0\", value=1)\n",
    "df_test.columns =['x0', 'x1', 'x2', 'x3', 'x4', 'x5', 'x6', 'x7', 'x8', 'x9', 'x10', 'sign']\n",
    "X_test = df_test.iloc[:, :-1].to_numpy()\n",
    "y_test = df_test.iloc[:, -1].to_numpy()\n",
    "\n",
    "X_pinv = np.linalg.pinv(X_train)\n",
    "w_lin = X_pinv.dot(y_train)\n",
    "\n",
    "# Define the classification error function\n",
    "def classification_error(X, y, w):\n",
    "    y_pred = np.sign(X.dot(w))\n",
    "    return np.mean(y_pred != y)\n",
    "\n",
    "# Compute the in-sample classification error\n",
    "Ein = classification_error(X_train, y_train, w_lin)\n",
    "# Compute the out-of-sample classification error\n",
    "Eout = classification_error(X_test, y_test, w_lin)\n",
    "\n",
    "result = np.abs(Ein - Eout)\n",
    "print(f\"Absolute gap between Ein and Eout: {result:.2f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Absolute gap between Ein and Eout: 0.08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-100-00b1bcd9a7cc>:11: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  new_X_train = np.hstack((X_train**(i+1) for i in range(2)))\n",
      "<ipython-input-100-00b1bcd9a7cc>:12: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  new_X_test = np.hstack((X_test**(i+1) for i in range(2)))\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv(\"ML_HW3_data_train.dat\", sep = \"\\t\", header = None)\n",
    "df_train.insert(loc=0, column=\"x0\", value=1)\n",
    "X_train = df_train.iloc[:, :-1].to_numpy()\n",
    "y_train = df_train.iloc[:, -1].to_numpy()\n",
    "\n",
    "df_test = pd.read_csv(\"ML_HW3_data_test.dat\", sep = \"\\t\", header = None)\n",
    "df_test.insert(loc=0, column=\"x0\", value=1)\n",
    "X_test = df_test.iloc[:, :-1].to_numpy()\n",
    "y_test = df_test.iloc[:, -1].to_numpy()\n",
    "\n",
    "new_X_train = np.hstack((X_train**(i+1) for i in range(2)))\n",
    "new_X_test = np.hstack((X_test**(i+1) for i in range(2)))\n",
    "\n",
    "X_pinv = np.linalg.pinv(new_X_train)\n",
    "w_lin = X_pinv.dot(y_train)\n",
    "\n",
    "# Define the classification error function\n",
    "def classification_error(X, y, w):\n",
    "    y_pred = np.sign(X.dot(w))\n",
    "    return np.mean(y_pred != y)\n",
    "\n",
    "# Compute the in-sample classification error\n",
    "Ein = classification_error(new_X_train, y_train, w_lin)\n",
    "# Compute the out-of-sample classification error\n",
    "Eout = classification_error(new_X_test, y_test, w_lin)\n",
    "\n",
    "result = np.abs(Ein - Eout)\n",
    "print(f\"Absolute gap between Ein and Eout: {result:.2f}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Absolute gap between Ein and Eout: 0.41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-101-612ed473908c>:11: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  new_X_train = np.hstack((X_train**(i+1) for i in range(8)))\n",
      "<ipython-input-101-612ed473908c>:12: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  new_X_test = np.hstack((X_test**(i+1) for i in range(8)))\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv(\"ML_HW3_data_train.dat\", sep = \"\\t\", header = None)\n",
    "df_train.insert(loc=0, column=\"x0\", value=1)\n",
    "X_train = df_train.iloc[:, :-1].to_numpy()\n",
    "y_train = df_train.iloc[:, -1].to_numpy()\n",
    "\n",
    "df_test = pd.read_csv(\"ML_HW3_data_test.dat\", sep = \"\\t\", header = None)\n",
    "df_test.insert(loc=0, column=\"x0\", value=1)\n",
    "X_test = df_test.iloc[:, :-1].to_numpy()\n",
    "y_test = df_test.iloc[:, -1].to_numpy()\n",
    "\n",
    "new_X_train = np.hstack((X_train**(i+1) for i in range(8)))\n",
    "new_X_test = np.hstack((X_test**(i+1) for i in range(8)))\n",
    "\n",
    "X_pinv = np.linalg.pinv(new_X_train)\n",
    "w_lin = X_pinv.dot(y_train)\n",
    "\n",
    "# Define the classification error function\n",
    "def classification_error(X, y, w):\n",
    "    y_pred = np.sign(X.dot(w))\n",
    "    return np.mean(y_pred != y)\n",
    "\n",
    "# Compute the in-sample classification error\n",
    "Ein = classification_error(new_X_train, y_train, w_lin)\n",
    "# Compute the out-of-sample classification error\n",
    "Eout = classification_error(new_X_test, y_test, w_lin)\n",
    "\n",
    "result = np.abs(Ein - Eout)\n",
    "print(f\"Absolute gap between Ein and Eout: {result:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
